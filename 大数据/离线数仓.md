# 离线数仓

## 自我介绍

我叫谢*，湖南人，今年24，做开发有四年了，2年的javaee，2年大数据。

上家是在平安健康险做javaee与大数据开关的工作。主要做的是互联网保险这块。

大数据这块，计算主要用：Flink/Spark/Hadoop/

日志数据采集用的是Flume，Canal

消息队列用的是Kafka，缓存用的Redis，

集群协调服务zk，

存储用到的是HBase+Phoenix，还有ES，Mysql

任务调度用的是Azkaban，集群监控，Zabbix

JavaEE主要是微服务开发，用到的技术：Spring Cloud系列，Mysql，Redis。

个人比较喜欢一些新的技术，业余的时候会研究一下，比如Docker，K8s，Pulsar，hudi，kudu。

## 部门组织

一个项目经理，一个大数据组长，3个离线数仓开发，2个实时开发，1个JavaEE开发，1个前端。

## 服务器

平安科技的服务器，12台，20核/40线程，128G内存，8THDD，2TSSD.

## 平台搭建

### 集群资源分配

- 分布式协调服务：3台zk

- HDFS：6台DataNode，2台NameNode，3台JM

- YARN：2台ResourceManager，6台NodeManager，日志服务器1台

- 消息队列：3台Kafka，加上监控

- NoSQL：3台ES，3台HBase，Redis3台

- RDBMS：2台MySQL

- 数据采集：1台Canal，2台Flueme

- 数据作业：2台Hive，2台Spark

- 任务调度：1台Azkaban

- 监控：所有机器部署

## 软件版本

| 名称        | 版本  |
| ----------- | ----- |
| Hadoop      | 2.7.2 |
| Kafka       | 0.11  |
| Kafka Eagal | 1.x   |
| HBase       | 1.3.4 |
| Redis       | 5.0   |
| ES          | 6.6   |
| Spark       | 2.4.5 |
| Hive        | 2.1   |
| Azkaban     | 2.5   |
| Flume       | 1.7   |
| Canal       | 1.x   |
| MySQL       | 5.7   |
| zk          | 3.4.5 |
| Spring Boot | 2.x   |



## 数仓搭建

### 数据量估算

#### 用户行为数据

1. 100万日活，每人每天100条，100万*100条=1亿条
2. 每条1kb，1亿/1kb≈100G
3. ODS存储，LZO压缩，十分之一等于10g
4. DWD，LZO+parquet，十分之一等于10G
5. DWS，轻度聚合，无压缩，50G，
6. ADS忽略不计
7. 副本*3等于210G
8. 半年内不扩容：210*180等于37T
9. 预留百分之20到30空间：37 / 0.7 = 53T

#### 业务数据

1. 100万日活，10万下单，每人10条，10万 * 10条  * 1kb=1G
2. 数仓四层：3g
3. 副本*3=9G
4. 半年：9 * 180=1.6T
5. 预留百分之20到30：1.6 / 0.7 = 2T

#### Kafka数据

1. 每天100G，2副本： 200G
2. 保持3天：600G
3. 预留百分之30：600 / 0.7 = 1T



合计：56T，约8T * 7台服务器。

### 数据用途

1. 报表展示
2. 机器学习
3. 用户画像
4. 风控系统
5. 推荐系统

### 数据源

1. 业务系统
   1. 客户从微信公众号，小程序，浏览器，H5等终端访问业务系统
   2. 请求经过Nginx反向代理至业务系统
   3. 业务系统的数据写入MySQL
   4. 离线数仓使用Azkaban调度Sqoop每日导出业务数据至HDFS
   5. 实时指标分析使用Canal将数据采集至Kafka
      1. 下游使用SparkStreaming执行实时指标分析，其间会使用Redis做数据去重
      2. 分析后的数据，写入ES，HBase+Phoenix
      3. 后续提供数据给java ee工程师开发Rest API，再由前端展示
2. 埋点系统
   1. 用户行为数据通过Nginx反向代理上传至日志服务器
   
   2. 日志服务器写入本地磁盘，保留30天
   
   3. 再由Flume采集至Kafka
   
   4. 下游再经过Flume消费，写入HDFS
3. 数据写入HDFS后，使用Hive进行数仓分层建模。
4. 建模完成后
   1. 使用Sqoop导出数据至MySQL，再用Superset做展示
   2. 使用Kylin进行预计算，做维度分析
   3. 使用Presto做即席查询
5. 集群其他组件
   1. ZK集群协调
   2. Zabbix+Granfa监控集群各个组件进行展示
   3. Kafka Eagal监控kafka
   4. ganglia监控Flume
   5. Atlas元数据管理
   6. Azkaban任务调度

### 数仓分层

1. ODS
   - 不做处理
   - 压缩
   - 分区
2. DWD
   - 空值去除
   - 过滤无意义数据
   - 维度退化
   - 数据脱敏
   - LZO压缩
   - 列存储
3. DWS
   - 每日轻量聚合
   - 列存储
   - 构建宽表
   - Kylin预计算
4. DWT
5. ADS
   - 指标分析
   - 文本存储
   - 数据导出
   - 报表展示

### 埋点系统数据

#### 启动日志

#### 事件日志

1. 商品列表
2. 商品点击
3. 商品详情
4. 商品广告
5. 消息通知
6. 用户后台活跃
7. 收藏
8. 点赞
9. 评论
10. 购物车
11. 错误日志

### 业务系统数据

#### 时间表

1. 时间表
2. 假期表
3. 假期年表

#### 业务表

1. 字典
2. 地区
3. 省份
4. 用户
5. 收藏
6. 点赞
7. 评论
8. 购物车
9. 订单
10. 订单详情
11. 订单状态
12. 支付
13. 退单
14. 优惠券
15. 优惠券规则
16. 优惠券领用
17. 活动
18. 活动订单
19. 活动商品表
20. SKU
21. SPU
22. 品牌
23. 三级分类
24. 二级分类
25. 一级分类

#### 维度建模

1. 确定业务线

2. 声明粒度

3. 确定维度

   1. 描述业务事实，时间，地点，人物，事情

4. 确认事实

   1. 指的是业务中的度量值，金额，次数，总数

   2. 以事实的角度看待维度

   3. |                      | ***\*时间\**** | ***\*用户\**** | ***\*地区\**** | ***\*商品\**** | ***\*优惠券\**** | ***\*活动\**** | ***\*编码\**** | ***\*度量值\**** |
      | -------------------- | -------------- | -------------- | -------------- | -------------- | ---------------- | -------------- | -------------- | ---------------- |
      | ***\*订单\****       | √              | √              | √              |                |                  | √              |                | 件数/金额        |
      | ***\*订单详情\****   | √              |                | √              | √              |                  |                |                | 件数/金额        |
      | ***\*支付\****       | √              |                | √              |                |                  |                |                | 金额             |
      | ***\*加购\****       | √              | √              |                | √              |                  |                |                | 件数/金额        |
      | ***\*收藏\****       | √              | √              |                | √              |                  |                |                | 个数             |
      | ***\*评价\****       | √              | √              |                | √              |                  |                |                | 个数             |
      | ***\*退款\****       | √              | √              |                | √              |                  |                |                | 件数/金额        |
      | ***\*优惠券领用\**** | √              | √              |                |                | √                |                |                | 个数             |

### 数仓数据表

#### ODS

埋点数据:

1. 启动日志
2. 事件日志

业务数据：

1. 字典
2. 地区
3. 省份
4. 日期
5. 假期
6. 年假期
7. 订单
8. 详情
9. 支付
10. 退款
11. 状态
12. 用户
13. 购物车
14. 收藏
15. 评论
16. 点赞
17. 活动
18. 活动订单
19. 活动规则
20. 优惠券
21. 优惠券领用
22. SKU
23. SPU
24. 品牌
25. 一级分类
26. 二级分类
27. 三级分类

#### DWD

1. 日志明细
2. 启动日志
3. 错误日志
4. 商品列表
5. 点击
6. 详情
7. 广告
8. 后台活跃
9. 推送
10. 点赞
11. 收藏
12. 评论
13. 购物车
14. 字典
15. 时间维度
16. 地区维度
17. 用户维度
18. 商品维度
19. 活动维度
20. 优惠券维度
21. 订单事实
22. 详情事实
23. 支付事实
24. 退款事实
25. 购物车事实
26. 收藏事实
27. 评论事实
28. 优惠券事实

#### DWS

1. 设备
2. 会员
3. 商品
4. 购买

#### DWT

1. 设备主题
2. 会员主题
3. 商品主题
4. 活动主题
5. 优惠券主题

#### ADS

1. 设备
   1. 日月周活
   2. 新增
   3. 沉默
   4. 流失
   5. 回流
   6. 留存
   7. 三周活跃
   8. 三天连续
2. 会员
   1. 会员主题分析
   2. 漏斗分析
3. 商品
   1. 个数
   2. 销量
   3. 购物车
   4. 差评
   5. 退款（30天）
4. 营销
   1. 下单
   2. 支付
   3. 回购

## 实时项目

#### 技术组件

1. Spark Streaming：实时计算
2. Redis：数据缓存，去重，过滤
3. ES：数据存储，灵活分析，指标图表展示
4. HBase+Phoenix：数据存储，提供数据，用于Rest API开发，再做前台展示

#### 指标需求

1. 日活指标：SparkStreaming消费Kafka启动日志主题，再利用Redis进行批次去重，最后数据写入HBase

2. GMV指标：通过Canal将订单表数据写入Kafka，再使用SparkStreaming对数据脱敏，写入HBase，再由Java EE开发Rest API进行统计

3. 报警指标：

   1. SparkStreaming消费Kafak事件日志主题，统计五分钟内，同一设备，连续三次不同账号登录，只领取消费券，不使用的用户
   2. 统计一小时内，连续下单三次，并退款三次的用户。

4. 灵活分析：

   1. 分析购买行为的男女比例，地域分布，年龄分布

   2. 将用户表提前加载至Redis缓存

   3. 使用SparkStreaming，进行流连接，关联订单表，详情表，用户表

   4. 将数据写入ES，后续供Java ee工程师开发Rest API接口，进行展示。

      

## 实际问题

### Flume

1. 小文件
   1. 设置写入次数统计为0
   2. 设置文件大小为128m生成新文件
   3. 设置一小时生成新文件
2. 无事务导致数据重复
   1. 下游去重
3. 数据积压
   1. 调整事务大小
   2. 调整批次大小
4. ganglia监控尝试提交次数远大于成功次数
   1. 增大内存
   2. 增加机器

### Kafka

1. 数据积压问题
   1. 增加分区数
   2. 增加消费者
   3. 消费者拉取数增大
2. 磁盘消费过快
   1. 调整副本保留时间
   2. 调整副本数
   3. 生产者压缩开启
3. 内存压力
   1. 调整启动参数
4. 网络问题导致集群压力增大
   1. 调整传输超时时间
5. 生产者数据无法发布，消费者无法消费
   1. 调整单个消息最大大小限制
   2. 调整消息复制限制

### Sqoop

1. Mysql null值问题
   1. 使用空值参数转换
2. 导出部分失败，一致性问题
   1. 使用临时表参数导出
3. 导出到Mysql，列存储问题
   1. 不要使用列存储
   2. 或者将parquet转换成文本
4. 数据倾斜问题
   1. 调整map task个数
   2. 使用split by指定分割字段
   3. 使用rownum函数均匀分布，再指定分割字段

### Hive

1. 数据倾斜
   1. 类型不匹配连接导致，连接时转换类型
   2. hot key导致，给key加随机数，使用多次连接处理
   3. 过滤空key，或者转换为随机数
   4. 使用group by替代distinct
   5. 开启数据倾斜负载均衡
2. 小文件问题
   1. har归档
   2. CombineHiveInputFormat合并小文件
   3. jvm重用
3. 字段分隔符问题
   1. 上游转义

### Spark

1. 数据倾斜
   1. 过滤掉倾斜key或使用两阶段聚合
   2. 提高shuffle并行度
   3. 使用预聚合算子，redubeByKey
   4. 随机key